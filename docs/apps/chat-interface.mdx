---
title: 'Agentic Chat Assistant'
description: 'Build a real-time agentic chat with interface streaming agent responses'
---

## How to build your own Agentic AI Assistant?

Create your own version of powerful AI assistants (similar to Claude Opus, Manus AI, 
Perplexity, Devin, or ChatGPT with plugins) - all using PandaAGI's SDK.

<img
  className="block dark:hidden"
  src="/images/chat_interface.png"
  alt="Chat Interface"
/>
<img
  className="hidden dark:block"
  src="/images/chat_interface.png"
  alt="Chat Interface"
/>

This example app demonstrates how to build a modern agentic chat with an interface that streams responses in real-time, showing agent thinking, tool usage, and providing a smooth user experience.

## Ready-to-use Agentic AI Assistant

A complete, enhanced agentic chat with an interface implementation is already available in the examples folder! You can get started immediately with our Docker-based setup.

The implemented solution includes:

- **Docker Integration**: One-command deployment with Docker Compose
- **Enhanced Event Handling**: Improved event filtering and error handling
- **Production Ready**: Nginx serving, proper CORS, health checks
- **Mobile Responsive**: Optimized for both desktop and mobile devices

### Quick Setup

```bash
cd examples/ui && ./start.sh
```

This one command will:
- ✅ Check Docker requirements  
- 🛑 Stop any existing containers
- 🔨 Build and start both services
- 🔍 Perform health checks
- 📊 Display service status and URLs

Once running, access:
- **Frontend**: http://localhost:3000
- **Backend API**: http://localhost:8001

This is the fastest way to get started with a production-ready agentic chat interface using the PandaAGI SDK.


### Alternative Setup

If you prefer manual setup or local development:

```bash
# Backend
cd examples/ui/backend && python main.py

# Frontend (in new terminal)
cd examples/ui/frontend && npm start
```

Congrats. Now you have your own production-ready agentic chat interface! Let's build the backend.


## FastAPI Server for Agentic Chat

You can wrap your own PandaAGI Agent in a simple backend server with streaming support:

```python
# backend/main.py
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import json
import asyncio
from typing import AsyncGenerator
import uuid

from panda_agi import Agent, EventType
from panda_agi.envs import DockerEnv

app = FastAPI(title="PandaAGI Chat API")

# Enable CORS for frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str
    conversation_id: str = None

async def event_stream(query: str, conversation_id: str = None) -> AsyncGenerator[str, None]:
    """Stream agent events as Server-Sent Events"""
    agent = None
    try:
        # Create environment for the agent
        agent_env = DockerEnv("./workspace")
        
        # Create agent with conversation ID for session persistence
        agent = Agent(
            environment=agent_env,
            conversation_id=conversation_id or str(uuid.uuid4())
        )
        
        # Stream events from the agent
        async for event in agent.run(query):
            # Convert event to dictionary for JSON serialization
            event_data = {
                "type": event.type.value,
                "timestamp": event.timestamp,
                "data": event.data
            }
            
            # Format as Server-Sent Event
            yield f"data: {json.dumps(event_data)}\n\n"
            
            # Add small delay to prevent overwhelming the client
            await asyncio.sleep(0.05)
            
    except Exception as e:
        # Send error event
        error_event = {
            "type": "error",
            "timestamp": "",
            "data": {"error": str(e)}
        }
        yield f"data: {json.dumps(error_event)}\n\n"
        
    finally:
        # Clean up agent connection
        if agent:
            try:
                await agent.disconnect()
            except Exception as e:
                print(f"Error disconnecting agent: {e}")

@app.post("/api/chat/stream")
async def stream_chat(request: ChatRequest):
    """Stream chat responses using Server-Sent Events"""
    
    return StreamingResponse(
        event_stream(request.message, request.conversation_id),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
        }
    )

@app.get("/api/health")
async def health_check():
    return {"status": "healthy"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### Requirements

```txt
# backend/requirements.txt
fastapi==0.104.1
uvicorn==0.24.0
panda-agi>=0.1.0
pydantic==2.5.0
```

## Start Building with PandaAGI

Ready to build your first AI-powered application? Check out our [quickstart guide](/quickstart) to learn how to integrate PandaAGI into your development workflow.

<CardGroup cols={2}>
  <Card
    title="Quickstart Guide"
    icon="play"
    href="/quickstart"
  >
    Learn how to install the SDK and build your first AI application
  </Card>
  <Card
    title="Architecture & Philosophy"
    icon="diagram-project"
    href="/concepts/architecture"
  >
    Understand PandaAGI's approach to AI development
  </Card>
</CardGroup> 